{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "path = '/content/drive/My Drive/workshop_texts/' # write your own path for fine-tuned models\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "MXEBhv6J7-ZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "tokenizer_path = 'ai-forever/rugpt3small_based_on_gpt2'\n",
        "model_path = path + 'big_book_fine_gpt_model'\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# model = GPT2LMHeadModel.from_pretrained(model_path).to(device) # to use fine-tuned model\n",
        "model = GPT2LMHeadModel.from_pretrained(tokenizer_path).to(device) # to use baseline model"
      ],
      "metadata": {
        "id": "32gP9MG_-Gi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_name = \"sarahai/ruT5-base-summarizer\"\n",
        "tokenizer_summarize = T5Tokenizer.from_pretrained(model_name)\n",
        "model_summarize = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhqFgucNmyq3",
        "outputId": "c45d8641-77c7-4ff6-eff9-4c5cae1b742b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(prompt, do_sample=True, num_beams=2, temperature=1.0, top_p=0.9, max_length=75): # TODO: play with hyperparameters\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(input_ids,\n",
        "                            do_sample=do_sample,\n",
        "                            num_beams=num_beams,\n",
        "                            temperature=temperature,\n",
        "                            top_p=top_p,\n",
        "                            max_length=max_length,\n",
        "                            repetition_penalty=1.6,\n",
        "                            )\n",
        "\n",
        "    return list(map(tokenizer.decode, out))[0]"
      ],
      "metadata": {
        "id": "WQ18NNJe-I3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "tokenizer_disc_path = 'cointegrated/rubert-tiny2'\n",
        "tokenizer_disc = BertTokenizer.from_pretrained(tokenizer_disc_path)\n",
        "\n",
        "model_disc_path = 'cointegrated/rubert-tiny2'\n",
        "model_disc = BertForSequenceClassification.from_pretrained(model_disc_path)\n",
        "model_disc = torch.load(path + 'BertClassifier2.pt', map_location=torch.device('cpu'))"
      ],
      "metadata": {
        "id": "k9nz_NkNLYzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51103d69-0aef-4d0c-f835-7cb7a06d34b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def get_score(text):\n",
        "    input_ids = tokenizer_disc(text, return_tensors=\"pt\")\n",
        "\n",
        "    model_disc.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model_disc(**input_ids)\n",
        "        logits = outputs.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    return int(np.argmax(logits, axis=1).flatten()[0])"
      ],
      "metadata": {
        "id": "_c7L1C0QObpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def generate_book(prompt, length=1000, chunk_length=200):\n",
        "    num_iter = length // chunk_length\n",
        "    last_line = \"\"\n",
        "    summary = prompt\n",
        "\n",
        "    for i in range(num_iter):\n",
        "      len_summary = len(re.findall(r'\\w+', last_line))\n",
        "      input_text = generate(summary, max_length=len_summary + chunk_length)\n",
        "\n",
        "      while get_score(input_text) < 2:\n",
        "          input_text = generate(summary, max_length=len_summary + chunk_length)\n",
        "\n",
        "      if input_text.find(last_line) != -1:\n",
        "          input_text = input_text[input_text.find(last_line) + len(last_line):]\n",
        "      print(input_text, end='')\n",
        "\n",
        "\n",
        "      input_text = input_text.splitlines()\n",
        "      input_text = [x for x in input_text if x]\n",
        "\n",
        "      #last_line = ' '.join(input_text[int(len(input_text)*0.8):])\n",
        "      #summary = last_line\n",
        "\n",
        "      last_line = input_text[-1]\n",
        "\n",
        "      input_text = \" \".join(input_text)\n",
        "\n",
        "      input_ids = tokenizer_summarize(input_text, return_tensors=\"pt\").input_ids.to(device)\n",
        "      outputs = model_summarize.generate(input_ids, max_length=50, min_length=25, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "      summary = tokenizer_summarize.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "      summary += last_line\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "KBv_InB6qOOF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_book(\"Глаза Степана Аркадьича весело заблестели, и он задумался, улыбаясь.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "9-I5GfTLlkoF",
        "outputId": "e313aaf9-e3bd-4aa2-b3ab-404aa6d24932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Глаза Степана Аркадьича весело заблестели, и он задумался, улыбаясь.\n",
            "\n",
            "– В чем же дело? – спросил он.\n",
            "\n",
            "– Да вот в чем… – Степан Аркадьич достал из кармана свой кисет и протянул его Степану Аркадьичу.\n",
            "\n",
            "– Ну, что ты хочешь сказать? – спросил Степан Аркадьич.\n",
            "\n",
            "Степан Аркадьич кивнул головой.\n",
            "\n",
            "– Ты знаешь, что за нами следят? – спросил он негромко.\n",
            "\n",
            "Степан Аркадьич не ответил.\n",
            "\n",
            "– Это я знаю, – сказал он.\n",
            "\n",
            "– Что?\n",
            "\n",
            "– То, что у нас есть.\n",
            "\n",
            "Степан Аркадьич смотрел на него с сомнением.\n",
            "\n",
            "– А ты откуда знаешь?\n",
            "\n",
            "– Я видел тебя вчера.\n",
            "\n",
            "– И что?\n",
            "\n",
            "– Видел.\n",
            "\n",
            "Степан Аркадьич молчал.\n",
            "\n",
            "– У меня есть друзья, – сказал он"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-159-93bdc892a3e1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_book\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Глаза Степана Аркадьича весело заблестели, и он задумался, улыбаясь.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-157-fee39458c23e>\u001b[0m in \u001b[0;36mgenerate_book\u001b[0;34m(prompt, length, chunk_length)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m       \u001b[0mlen_summary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\w+'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen_summary\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       '''while get_score(input_text) < 2:\n",
            "\u001b[0;32m<ipython-input-156-2069425b1a5f>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(prompt, do_sample, num_beams, temperature, top_p, max_length)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         out = model.generate(input_ids,\n\u001b[0m\u001b[1;32m      8\u001b[0m                             \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdo_sample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                             \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_beams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1664\u001b[0m             \u001b[0;31m# 14. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1665\u001b[0;31m             return self.beam_sample(\n\u001b[0m\u001b[1;32m   1666\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m                 \u001b[0mbeam_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mbeam_sample\u001b[0;34m(self, input_ids, beam_scorer, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, output_logits, return_dict_in_generate, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m             )\n\u001b[1;32m   3489\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m                 model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n\u001b[0m\u001b[1;32m   3491\u001b[0m                     \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"past_key_values\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_temporary_reorder_cache\u001b[0;34m(self, past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   2803\u001b[0m         \u001b[0;31m# Exception 1: code path for models using the legacy cache format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2804\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2805\u001b[0;31m             \u001b[0mpast_key_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reorder_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2806\u001b[0m         \u001b[0;31m# Exception 2: models with different cache formats. These are limited to `DynamicCache` until their\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2807\u001b[0m         \u001b[0;31m# cache format is standardized, to avoid adding complexity to the codebase.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_reorder_cache\u001b[0;34m(past_key_values, beam_idx)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0mbeam_idx\u001b[0m \u001b[0mat\u001b[0m \u001b[0mevery\u001b[0m \u001b[0mgeneration\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \"\"\"\n\u001b[0;32m-> 1131\u001b[0;31m         return tuple(\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \"\"\"\n\u001b[1;32m   1131\u001b[0m         return tuple(\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \"\"\"\n\u001b[1;32m   1131\u001b[0m         return tuple(\n\u001b[0;32m-> 1132\u001b[0;31m             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpast_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_past\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1133\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer_past\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(\"Глаза Степана Аркадьича весело заблестели, и он задумался, улыбаясь.\", max_length=300))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frJVescGTv3S",
        "outputId": "ef653f82-279d-4fb3-9652-4c9451c7f700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Глаза Степана Аркадьича весело заблестели, и он задумался, улыбаясь.\n",
            "\n",
            "— Да, да, — подтвердил Степан Аркадьевич. — Я вижу, что вы хорошо поработали.\n",
            "\n",
            "— Так точно, — подтвердил я.\n",
            "\n",
            "— Ну, а теперь скажите мне, пожалуйста, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — ответил я.\n",
            "\n",
            "— Вот видите! — воскликнул Степан Аркадьевич. — А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — ответил я.\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — сказал я.\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — сказал я.\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — сказал я.\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — сказал я.\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — сказал я.\n",
            "\n",
            "— А теперь скажите мне, зачем вы это сделали?\n",
            "\n",
            "— Не знаю, — сказал я.\n",
            "\n",
            "— А\n"
          ]
        }
      ]
    }
  ]
}